{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /home/haku/.local/lib/python3.10/site-packages (2.0.2)\n",
      "Requirement already satisfied: seaborn in /home/haku/.local/lib/python3.10/site-packages (0.12.2)\n",
      "Requirement already satisfied: scikit-learn in /home/haku/.local/lib/python3.10/site-packages (1.2.2)\n",
      "Requirement already satisfied: scikit-plot in /home/haku/.local/lib/python3.10/site-packages (0.3.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/haku/.local/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/haku/.local/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/haku/.local/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /home/haku/.local/lib/python3.10/site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in /home/haku/.local/lib/python3.10/site-packages (from seaborn) (3.7.1)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /home/haku/.local/lib/python3.10/site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/haku/.local/lib/python3.10/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/haku/.local/lib/python3.10/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/haku/.local/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/haku/.local/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/haku/.local/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.39.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/haku/.local/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/haku/.local/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/haku/.local/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.4.7)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas seaborn scikit-learn scikit-plot\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.modules import activation, dropout, batchnorm\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import copy\n",
    "from collections import Counter, defaultdict, deque, OrderedDict\n",
    "from collections.abc import Iterable\n",
    "from itertools import chain, combinations\n",
    "from pyod.models.sos import SOS\n",
    "import argparse\n",
    "from Net import CNN_UNSW\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import math\n",
    "import inspect\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "import traceback\n",
    "from hprofile import Profile, jaccard_simple\n",
    "from utils import TorchHook, DDPCounter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert a list of list to a list [[],[],[]]->[,,]\n",
    "def flatten(items):\n",
    "    \"\"\"Yield items from any nested iterable; see Reference.\"\"\"\n",
    "    for x in items:\n",
    "        if isinstance(x, Iterable) and not isinstance(x, (str, bytes)):\n",
    "            for sub_x in flatten(x):\n",
    "                yield sub_x\n",
    "        else:\n",
    "            yield x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readdataset():\n",
    "    normalized_X = np.load('clnt_x.npy')\n",
    "    y = np.load('clnt_y.npy')\n",
    "    print('y', sorted(Counter(y).items()))\n",
    "\n",
    "    # downsampling = RandomUnderSampler(\n",
    "    #     sampling_strategy={0: 100000, 1: 100000, 2: 44525, 3: 24246, 4: 16353, 5: 13987, 6: 0, 7: 0, 8: 0, 9: 0},\n",
    "    #     random_state=0)\n",
    "    downsampling = RandomUnderSampler(\n",
    "        sampling_strategy={0: 400000, 1: 100000, 2: 44525, 3: 24246, 4: 16353, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0},\n",
    "        random_state=0)\n",
    "    X_down, y_down = downsampling.fit_resample(normalized_X, y)\n",
    "    # upsampling = RandomOverSampler(\n",
    "    #     sampling_strategy={0: 100000, 1: 100000, 2: 100000, 3: 100000, 4: 100000, 5: 100000}, random_state=0)\n",
    "    upsampling = RandomOverSampler(\n",
    "        sampling_strategy={0: 400000, 1: 100000, 2: 100000, 3: 100000, 4: 100000}, random_state=0)\n",
    "    Xt, yt = upsampling.fit_resample(X_down, y_down)\n",
    "    print('transformed y', sorted(Counter(yt).items()))\n",
    "    df = pd.DataFrame(Xt, index=yt)\n",
    "    df.sort_index(ascending=True, inplace=True)\n",
    "    train0 = df.iloc[0:280000]\n",
    "    train1 = df.iloc[400000:400000 + 70000]\n",
    "    train2 = df.iloc[500000:500000 + 70000]\n",
    "    train3 = df.iloc[600000:600000 + 70000]\n",
    "    train4 = df.iloc[700000:700000 + 70000]\n",
    "    print(train0.shape, train1.shape, train2.shape, train3.shape, train4.shape)\n",
    "    # train5 = df.iloc[500000:500000 + 70000]\n",
    "    df_train = pd.concat([train0, train1, train2, train3, train4])  # , train5\n",
    "    df_train = shuffle(df_train)\n",
    "    np_features_train = df_train.values\n",
    "\n",
    "    np_features_train = np_features_train[:, np.newaxis, :]\n",
    "    np_label_train = df_train.index.values.ravel()\n",
    "    print('train', sorted(Counter(np_label_train).items()))\n",
    "\n",
    "    test0 = df.iloc[280000:400000]\n",
    "    test1 = df.iloc[400000 + 70000:400000 + 100000]\n",
    "    test2 = df.iloc[500000 + 70000:500000 + 100000]\n",
    "    test3 = df.iloc[600000 + 70000:600000 + 100000]\n",
    "    test4 = df.iloc[700000 + 70000:700000 + 100000]\n",
    "    # test5 = df.iloc[500000 + 70000:500000 + 100000]\n",
    "    print(test0.shape, test1.shape, test2.shape, test3.shape, test4.shape)\n",
    "    \n",
    "    df_test = pd.concat([test0, test1, test2, test3, test4])  # , test5\n",
    "    df_test = shuffle(df_test)\n",
    "    features_test = df_test.values\n",
    "    np_features_test = np.array(features_test)\n",
    "\n",
    "    np_features_test = np_features_test[:, np.newaxis, :]\n",
    "    np_label_test = df_test.index.values.ravel()\n",
    "    print('test', sorted(Counter(np_label_test).items()))\n",
    "    return np_features_train, np_label_train, np_features_test, np_label_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadData(Dataset):\n",
    "    def __init__(self, x_tra, y_tra):\n",
    "        self.x_train = x_tra\n",
    "        self.y_train = y_tra\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_train)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image, label = self.x_train[item], self.y_train[item]\n",
    "        image = torch.from_numpy(image)\n",
    "        label = torch.from_numpy(np.asarray(label))\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetSplit(Dataset):\n",
    "    def __init__(self, dataset, idxs):\n",
    "        self.dataset = dataset\n",
    "        self.idxs = list(idxs)\n",
    "        self.features, self.labels = self.dataset[self.idxs]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idxs)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image, label = self.dataset[self.idxs[item]]\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchProfiler():\n",
    "\n",
    "    def __init__(self, model, device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')):\n",
    "\n",
    "        super().__init__()\n",
    "        self.activation_classes = [m[1] for m in inspect.getmembers(activation, inspect.isclass) if\n",
    "                                   m[1].__module__ == 'torch.nn.modules.activation']  \n",
    "        self.dropout_classes = [m[1] for m in inspect.getmembers(dropout, inspect.isclass) if\n",
    "                                m[1].__module__ == 'torch.nn.modules.dropout']  \n",
    "        self.batchnorm_classes = [m[1] for m in inspect.getmembers(batchnorm, inspect.isclass) if\n",
    "                                  m[1].__module__ == 'torch.nn.modules.batchnorm']\n",
    "        self.implemented_classes = [torch.nn.Linear,\n",
    "                                    torch.nn.MaxPool1d,\n",
    "                                    torch.nn.AdaptiveAvgPool1d,\n",
    "                                    torch.nn.Conv1d]\n",
    "        self.contrib_functions = ['_contrib_linear',\n",
    "                                  '_contrib_max1d',\n",
    "                                  '_contrib_adaptive_avg_pool1d',\n",
    "                                  '_contrib_conv1d']\n",
    "\n",
    "        self.model = TorchHook(model)\n",
    "        self.hooks = self.model.available_modules()\n",
    "\n",
    "    def create_layers(self, nlayers=0):\n",
    "        hooks = self.hooks\n",
    "        if nlayers == 0:\n",
    "            # this will be greater than we need because activation layers will join with implemented layers.\n",
    "            nlayers = len(hooks)\n",
    "        namelist = set()\n",
    "\n",
    "        layerdict = OrderedDict()\n",
    "\n",
    "        revhooks = reversed(hooks)\n",
    "        layeridx = DDPCounter(start=0)\n",
    "        tmplyer = deque()\n",
    "        for kdx in revhooks:\n",
    "            if layeridx() == nlayers:\n",
    "                tmplyer.appendleft(kdx)\n",
    "                namelist.add(kdx)\n",
    "                layeridx.inc()\n",
    "                layerdict[layeridx()] = [list(tmplyer), 0]\n",
    "                break\n",
    "            this_type = type(hooks[kdx])\n",
    "            if this_type in self.dropout_classes:\n",
    "                continue\n",
    "            elif this_type in self.activation_classes:\n",
    "                tmplyer.appendleft(kdx)\n",
    "                namelist.add(kdx)\n",
    "                continue\n",
    "            elif this_type in self.batchnorm_classes:\n",
    "                continue\n",
    "            elif this_type in self.implemented_classes:\n",
    "                tmplyer.appendleft(kdx)\n",
    "                namelist.add(kdx)\n",
    "                layeridx.inc()\n",
    "                layerdict[layeridx()] = [list(tmplyer),\n",
    "                                         self.contrib_functions[self.implemented_classes.index(this_type)]]\n",
    "                tmplyer = deque()\n",
    "                continue\n",
    "            else:\n",
    "                print(f'profiler not implemented for layer of type: {type(hooks[kdx])}')\n",
    "                tmplyer.appendleft(kdx)\n",
    "                namelist.add(kdx)\n",
    "                layeridx.inc()\n",
    "                layerdict[layeridx()] = [list(tmplyer), 0]\n",
    "                break\n",
    "        else:\n",
    "            layeridx.inc()\n",
    "            layerdict[layeridx()] = [0, 0]\n",
    "        namelist = list(namelist)\n",
    "        if len(namelist) > 0:\n",
    "            self.model.add_hooks(namelist)\n",
    "        return layerdict\n",
    "\n",
    "    def _single_profile(self, x_in, y_out, R, layers, layerdict, ldx, threshold):\n",
    "        func = getattr(self.__class__, layerdict[ldx][1])\n",
    "        return func(self, x_in, y_out, R, layers, threshold)\n",
    "\n",
    "    def create_profile(self, x, layerdict, n_layers=0, threshold=0.5, show_progress=False, parallel=False):\n",
    "        x = x.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y, actives = self.model.forward(x)\n",
    "\n",
    "            neuron_counts = defaultdict(list)\n",
    "            synapse_counts = defaultdict(Counter)\n",
    "            synapse_weights = defaultdict(list)\n",
    "\n",
    "            # initialize profile with index of maximal logit from last layer\n",
    "            neuron = int((torch.argmax(y[0].cpu())).detach().numpy())\n",
    "            neuron_counts[0].append(neuron)\n",
    "            synapse_counts[0].update([(neuron, neuron, 0)])\n",
    "            synapse_weights[0].append(torch.max(y[0].cpu()))\n",
    "            mask = torch.zeros_like(y.cpu())\n",
    "            mask[:, torch.argmax(y.cpu())] = 1\n",
    "            R = y.cpu() * mask\n",
    "\n",
    "            if n_layers == 0 or n_layers >= len(layerdict):\n",
    "                n = len(layerdict)\n",
    "            else:\n",
    "                n = n_layers + 1\n",
    "            for ldx in range(1, n):\n",
    "                try:\n",
    "                    if show_progress:\n",
    "                        print(f'Layer #{ldx}')\n",
    "                    inlayers, incontrib = layerdict[ldx + 1]\n",
    "                    if incontrib == 0 and inlayers == 0:\n",
    "                        x_in = x\n",
    "                    else:\n",
    "                        x_in = actives[inlayers[-1]]\n",
    "\n",
    "                    # next retrieve y_out\n",
    "                    layers, contrib = layerdict[ldx]\n",
    "                    y_out = actives[layers[-1]]\n",
    "\n",
    "                    nc, sc, sw, Rx = self._single_profile(x_in, y_out, R, layers, layerdict, ldx, threshold)\n",
    "                    neuron_counts[ldx].append(nc)\n",
    "                    synapse_counts[ldx].update(sc)\n",
    "                    synapse_weights[ldx].append(sw)\n",
    "\n",
    "                    R = Rx\n",
    "\n",
    "                except Exception as ex:\n",
    "                    traceback.print_exc()\n",
    "                    break\n",
    "\n",
    "            return Profile(neuron_counts=neuron_counts,\n",
    "                           synapse_counts=synapse_counts,\n",
    "                           synapse_weights=synapse_weights, num_inputs=1)\n",
    "\n",
    "    def _contrib_max1d(self, x_in, y_out, R, layer, threshold=0.001):\n",
    "\n",
    "        neuron_counts = list()\n",
    "        synapse_counts = Counter()\n",
    "        synapse_weights = list()\n",
    "        maxpool = self.model.available_modules()[layer[0]]\n",
    "\n",
    "        # Grab dimensions of maxpool from parameters\n",
    "        stride = maxpool.stride\n",
    "        kernel_size = maxpool.kernel_size\n",
    "        #         Rx = torch.zeros_like(x_in)\n",
    "\n",
    "        tmp_return_indices = bool(maxpool.return_indices)\n",
    "        maxpool.return_indices = True\n",
    "        _, indices = maxpool.forward(x_in)\n",
    "        maxpool.return_indices = tmp_return_indices\n",
    "        Rx = torch.nn.functional.max_unpool1d(input=R, indices=indices.cpu(), kernel_size=kernel_size, stride=stride,\n",
    "                                              padding=maxpool.padding, output_size=x_in.shape)\n",
    "\n",
    "\n",
    "        Rx_sum = torch.sum(Rx, dim=2)\n",
    "        K = int(threshold * len(Rx_sum.view(-1)))\n",
    "        TOPK_value_index = torch.topk(Rx_sum.view(-1), K)\n",
    "        neuron_counts.append(TOPK_value_index[1].tolist())\n",
    "\n",
    "        return neuron_counts, synapse_counts, synapse_weights, Rx\n",
    "\n",
    "    def _contrib_adaptive_avg_pool1d(self, x_in, y_out, R, layer, threshold=0.001):\n",
    "\n",
    "        neuron_counts = list()\n",
    "        synapse_counts = Counter()\n",
    "        synapse_weights = list()\n",
    "        avgpool = self.model.available_modules()[layer[0]]\n",
    "\n",
    "        '''Grab the dimensions used by an adaptive pooling layer'''\n",
    "        output_size = avgpool.output_size[0]\n",
    "        input_size = x_in.shape[-1]\n",
    "        stride = (input_size // output_size)\n",
    "        kernel_size = input_size - (output_size - 1) * stride\n",
    "        Rx = torch.zeros_like(x_in).cpu()  ###, dtype=np.float\n",
    "\n",
    "        for i in range(R.size(2)):\n",
    "            for j in range(R.size(3)):\n",
    "                Z = x_in[:, :, i * stride:i * stride + kernel_size, j * stride:j * stride + kernel_size].cpu()\n",
    "                Zs = Z.sum(axis=(2, 3), keepdims=True)\n",
    "                Zs += 1e-12 * ((Zs >= 0).float() * 2 - 1)\n",
    "                Rx[:, :, i * stride:i * stride + kernel_size, j * stride:j * stride + kernel_size] += (\n",
    "                        (Z / Zs) * R[:, :, i:i + 1, j:j + 1])\n",
    "\n",
    "        Rx_sum = torch.sum(Rx, dim=2)\n",
    "        K = int(threshold * len(Rx_sum.view(-1)))\n",
    "        TOPK_value_index = torch.topk(Rx_sum.view(-1), K)\n",
    "        neuron_counts.append(TOPK_value_index[1].tolist())\n",
    "\n",
    "        return neuron_counts, synapse_counts, synapse_weights, Rx\n",
    "\n",
    "    def _contrib_conv1d(self, x_in, y_out, R, layers,\n",
    "                        threshold=0.001):  ###(self, x_in, y_out, ydx, layer, threshold=0.1)\n",
    "        threshold = 0.1\n",
    "        neuron_counts = list()\n",
    "        synapse_counts = Counter()\n",
    "        synapse_weights = list()\n",
    "        conv, actf = layers \n",
    "        conv = self.model.available_modules()[conv]\n",
    "        actf = self.model.available_modules()[actf]\n",
    "\n",
    "        # assumption is that kernel size, stride are equal in both dimensions\n",
    "        # and padding preserves input size\n",
    "        kernel_size = conv.kernel_size[0]\n",
    "        stride = conv.stride[0]\n",
    "        padding = conv.padding[0]\n",
    "        W = conv._parameters['weight']\n",
    "        B = conv._parameters['bias']\n",
    "       \n",
    "        Z = torch.nn.functional.conv1d(x_in, weight=W, bias=None, stride=stride, padding=padding).cpu()\n",
    "        S = R / (Z + 1e-16 * ((Z >= 0).float() * 2 - 1.))\n",
    "        C = torch.nn.functional.conv_transpose1d(input=S, weight=W.cpu(), bias=None, stride=stride, padding=padding)\n",
    "        Rx = C * x_in.cpu()\n",
    "\n",
    "        Rx_sum = torch.sum(Rx, dim=2)\n",
    "        K = int(threshold * len(Rx_sum.view(-1)))\n",
    "        TOPK_value_index = torch.topk(Rx_sum.view(-1), K)\n",
    "        neuron_counts.append(TOPK_value_index[1].tolist())\n",
    "\n",
    "        return neuron_counts, synapse_counts, synapse_weights, Rx\n",
    "\n",
    "    def _contrib_linear(self, x_in, y_out, R, layers, threshold=0.0001):\n",
    "        threshold = 0.1\n",
    "        neuron_counts = list()\n",
    "        synapse_counts = Counter()\n",
    "        synapse_weights = list()\n",
    "        Rx = torch.zeros_like(x_in).cpu()\n",
    "\n",
    "        if len(layers) == 1:\n",
    "            linear = layers[0]\n",
    "\n",
    "            def actf(x):\n",
    "                return x\n",
    "        else:\n",
    "            linear, actf = layers\n",
    "            actf = self.model.available_modules()[actf]\n",
    "        linear = self.model.available_modules()[linear]\n",
    "\n",
    "        xshape = x_in.shape\n",
    "        xdims = x_in[0].shape\n",
    "        if len(xdims) > 1:\n",
    "            holdx = torch.Tensor(x_in.cpu())\n",
    "            x_in = x_in[0].view(-1).unsqueeze(0)\n",
    "\n",
    "        W = linear._parameters['weight']\n",
    "        B = linear._parameters['bias']\n",
    "\n",
    "        Z = torch.nn.functional.linear(x_in, W, bias=None).cpu()\n",
    "        S = R / (Z + 1e-16 * ((Z >= 0).float() * 2 - 1.))\n",
    "        Rx = torch.nn.functional.linear(S, W.t().cpu(), bias=None)\n",
    "        Rx *= x_in.cpu()\n",
    "        Rx = Rx.reshape(xshape)\n",
    "\n",
    "        K = int(threshold * len(Rx.view(-1)))\n",
    "        TOPK_value_index = torch.topk(Rx.view(-1), K)\n",
    "        neuron_counts.append(TOPK_value_index[1].tolist())\n",
    "\n",
    "        return neuron_counts, synapse_counts, synapse_weights, Rx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iid(dataset, num_users, degree):\n",
    "    num_normal = 280000 // num_users\n",
    "    num_attack = 280000 // (num_users * degree)\n",
    "    dict_users = {i: np.array([], dtype='int64') for i in range(num_users)}\n",
    "    idxs = np.arange(280000 * 2)\n",
    "    labels = dataset.y_train\n",
    "    # sort labels\n",
    "    idxs_labels = np.vstack((idxs, labels))  ###[[idxs 0,1,2,3],[labels 5,5,7,2]]\n",
    "    idxs_labels = idxs_labels[:, idxs_labels[1, :].argsort()]\n",
    "    idxs = idxs_labels[0, :]  ### idxs前224000为正常类样本的index，后面每116000为下一类\n",
    "    dict_class_index = {}  # {i: [] for i in range(7)}\n",
    "    dict_class_index[0] = idxs[0:280000]\n",
    "    for i in range(1, 5):\n",
    "        dict_class_index[i] = idxs[280000 + (i - 1) * 70000:280000 + i * 70000]\n",
    "    comb = list()\n",
    "    for i in range(int(math.ceil(num_users / len(list(combinations([i for i in range(1, 5)], degree)))))):\n",
    "        comb += list(combinations([i for i in range(1, 5)], degree))\n",
    "    # comb_rand = random.sample(comb, 100)\n",
    "    comb_rand = comb[0:100]\n",
    "    print('comb', len(comb_rand))\n",
    "    for i, classes in enumerate(comb_rand):\n",
    "        # rand_set_normal = np.random.choice(dict_class_index[0], num_normal, replace=False)\n",
    "        rand_set_normal = dict_class_index[0][0:num_normal]\n",
    "        dict_users[i] = np.concatenate((dict_users[i], rand_set_normal), axis=0)\n",
    "        dict_class_index[0] = list(set(dict_class_index[0]) - set(rand_set_normal))\n",
    "        for cls in classes:\n",
    "            if len(dict_class_index[cls]) >= num_attack:\n",
    "                # rand_set_attack = np.random.choice(dict_class_index[cls], num_attack, replace=False)\n",
    "                rand_set_attack = dict_class_index[cls][0:num_attack]\n",
    "                dict_users[i] = np.concatenate((dict_users[i], rand_set_attack), axis=0)\n",
    "                dict_class_index[cls] = list(set(dict_class_index[cls]) - set(rand_set_attack))\n",
    "            else:\n",
    "                dict_users[i] = np.concatenate((dict_users[i], dict_class_index[cls]), axis=0)\n",
    "    return dict_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_img(net_g, datatest):\n",
    "    net_g.eval()\n",
    "    # testing\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    data_pred = []\n",
    "    data_label = []\n",
    "    x = datatest.x_train\n",
    "    y = datatest.y_train\n",
    "    anomaly_list = [i for i in range(len(y)) if y[i] != 0]\n",
    "    y[anomaly_list] = 1\n",
    "    dataset_test = ReadData(x, y)\n",
    "    data_loader = DataLoader(dataset_test, batch_size=test_BatchSize)\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "    for idx, (data, target) in enumerate(data_loader):\n",
    "        data, target = Variable(data).to(device), Variable(target).type(torch.LongTensor).to(device)\n",
    "        # data, target = Variable(data), Variable(target).type(torch.LongTensor)\n",
    "        log_probs = net_g(data)\n",
    "        # sum up batch loss\n",
    "        test_loss += loss(log_probs, target).item()\n",
    "        # test_loss += F.cross_entropy(log_probs, target, reduction='sum').item()\n",
    "        # get the index of the max log-probability\n",
    "        y_pred = log_probs.data.detach().max(1, keepdim=True)[1]\n",
    "        correct += y_pred.eq(target.data.detach().view_as(y_pred)).long().cpu().sum()\n",
    "        data_pred.append(y_pred.cpu().detach().data.tolist())\n",
    "        data_label.append(target.cpu().detach().data.tolist())\n",
    "    list_data_label = list(flatten(data_label))\n",
    "    list_data_pred = list(flatten(data_pred))\n",
    "    print(classification_report(list_data_label, list_data_pred))\n",
    "    print(confusion_matrix(list_data_label, list_data_pred))\n",
    "    print('test_loss', test_loss)\n",
    "    test_loss /= len(data_loader.dataset)\n",
    "    accuracy = 100.00 * correct / len(data_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f} \\nAccuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss, correct, len(data_loader.dataset), accuracy))\n",
    "    return accuracy, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_w(w, datatest):\n",
    "    net_w = CNN_UNSW().double().to(device)\n",
    "    net_w.load_state_dict(w)\n",
    "    net_w.eval()\n",
    "    # testing\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    data_pred = []\n",
    "    data_label = []\n",
    "    x = datatest.x_train\n",
    "    y = datatest.y_train\n",
    "    anomaly_list = [i for i in range(len(y)) if y[i] != 0]\n",
    "    y[anomaly_list] = 1\n",
    "    dataset_test = ReadData(x, y)\n",
    "    data_loader = DataLoader(dataset_test, batch_size=test_BatchSize)\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "    for idx, (data, target) in enumerate(data_loader):\n",
    "        data, target = Variable(data).to(device, dtype=torch.double), Variable(target).type(torch.LongTensor).to(device)\n",
    "        # data, target = Variable(data), Variable(target).type(torch.LongTensor)\n",
    "        log_probs = net_w(data)\n",
    "        # sum up batch loss\n",
    "        test_loss += loss(log_probs, target).item()\n",
    "        # test_loss += F.cross_entropy(log_probs, target, reduction='sum').item()\n",
    "        # get the index of the max log-probability\n",
    "        y_pred = log_probs.data.detach().max(1, keepdim=True)[1]\n",
    "        correct += y_pred.eq(target.data.detach().view_as(y_pred)).long().cpu().sum()\n",
    "        data_pred.append(y_pred.cpu().detach().data.tolist())\n",
    "        data_label.append(target.cpu().detach().data.tolist())\n",
    "    list_data_label = list(flatten(data_label))\n",
    "    list_data_pred = list(flatten(data_pred))\n",
    "    print(classification_report(list_data_label, list_data_pred))\n",
    "    print(confusion_matrix(list_data_label, list_data_pred))\n",
    "    # print('test_loss', test_loss)\n",
    "    test_loss /= len(data_loader.dataset)\n",
    "    accuracy = 100.00 * correct / len(data_loader.dataset)\n",
    "    return accuracy, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FedAvg(w):\n",
    "    w_avg = copy.deepcopy(w[0])\n",
    "    for k in w_avg.keys():\n",
    "        for i in range(1, len(w)):\n",
    "            w_avg[k] += w[i][k]\n",
    "        w_avg[k] = torch.div(w_avg[k], len(w))\n",
    "    return w_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGradVec(w):\n",
    "    \"\"\"Return the gradient flattened to a vector\"\"\"\n",
    "    gradVec = []\n",
    "    for k in w.keys():\n",
    "        gradVec.append(w[k].view(-1).float())\n",
    "    # concat into a single vector\n",
    "    gradVec = torch.cat(gradVec).cpu().numpy()\n",
    "    return gradVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def defence_det(w, d_out):\n",
    "    w_avg = copy.deepcopy(w[0])\n",
    "    for k in w_avg.keys():\n",
    "        for i in range(1, len(w)):\n",
    "            ### 0:clean model，1:poisoned model\n",
    "            if d_out[i] == 0:\n",
    "                w_avg[k] += w[i][k]\n",
    "        w_avg[k] = torch.div(w_avg[k], (len(d_out) - sum(d_out)))\n",
    "    return w_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def defence_our(omega_locals, w_locals, w_local_pre):\n",
    "    X_norm = []\n",
    "    selected_index = {}\n",
    "    for i in omega_locals[0].keys():\n",
    "        aggregate_index = list()\n",
    "        for j in range(0, len(omega_locals)):\n",
    "            aggregate_index.append(omega_locals[j][i])\n",
    "        selected_index[i] = Counter(list(chain(*aggregate_index)))\n",
    "    print('interation', interation, 'client', client)\n",
    "    # print('selected_index', selected_index)\n",
    "\n",
    "    for i in range(0, len(w_locals)):\n",
    "        selected_weights = []\n",
    "        all_weights = []\n",
    "        for n in w_locals[0].keys():\n",
    "            # print(n)\n",
    "            c = w_locals[i][n].cpu()\n",
    "            c_pre = w_local_pre[n].cpu()\n",
    "            # all_weights.append((c.view(-1).detach().numpy() - c_pre.view(-1).detach().numpy()))\n",
    "            selected_index_dict = dict(selected_index[n])\n",
    "            indice = []\n",
    "            for a in range(0, len(selected_index_dict)):\n",
    "                # if (list(selected_index_dict.values())[a] < 45)&(list(selected_index_dict.values())[a] >40):\n",
    "                if (list(selected_index_dict.values())[a] > 90):\n",
    "                    # if ((list(selected_index_dict.values())[a] > 30) & (list(selected_index_dict.values())[a] < 40)): # | (list(selected_index_dict.values())[a] > 95)\n",
    "                    indice.append(list(selected_index_dict.keys())[a])\n",
    "            if len(indice) > 0:\n",
    "                indices = torch.tensor(indice)\n",
    "                # print('indices', indices)\n",
    "                d = torch.index_select(c.view(-1), 0, indices)\n",
    "                d_pre = torch.index_select(c_pre.view(-1), 0, indices)\n",
    "                selected_weights.append((d.view(-1).detach().numpy() - d_pre.view(-1).detach().numpy()))\n",
    "            else:\n",
    "                pass\n",
    "        X_norm.append(list(chain(*selected_weights)))\n",
    "        # X_all.append(list(chain(*all_weights)))\n",
    "\n",
    "    X_norm = np.array(X_norm)\n",
    "    # X_all = np.array(X_all)\n",
    "    print('X', X_norm.shape)\n",
    "    #### OUR\n",
    "    scaler = MinMaxScaler()\n",
    "    # scaler = StandardScaler()\n",
    "    scaler.fit(X_norm)\n",
    "    X_norm = scaler.transform(X_norm)\n",
    "    # outliers_fraction = float(num_poison_client/num_clients)\n",
    "    # print('######outliers_fraction',outliers_fraction)\n",
    "    random_state = 42\n",
    "    clf = SOS(contamination=0.4, perplexity=90)\n",
    "\n",
    "    clf.fit(X_norm)\n",
    "    pre_out_label = clf.labels_\n",
    "    print('prediction', pre_out_label)\n",
    "    print(confusion_matrix(Y_norm.astype(int), pre_out_label))\n",
    "    print(classification_report(Y_norm.astype(int), pre_out_label))\n",
    "    # print(\"train AC\", accuracy_score(Y_norm.astype(int), pre_out_label))\n",
    "    ### Federated aggregation with defence\n",
    "    w_glob = defence_det(w_locals, pre_out_label)\n",
    "    return w_glob, pre_out_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidate(Model, Weight, MEAN_pre, epsilon):\n",
    "    OMEGA_current = {n: p.data.clone().zero_() for n, p in Model.named_parameters()}\n",
    "    for n, p in Model.named_parameters():\n",
    "        p_current = p.detach().clone()\n",
    "        p_change = p_current - MEAN_pre[n]\n",
    "        # W[n].add_((p.grad**2) * torch.abs(p_change))\n",
    "        # OMEGA_add = W[n]/ (p_change ** 2 + epsilon)\n",
    "        # W[n].add_(-p.grad * p_change)\n",
    "        OMEGA_add = torch.max(Weight[n], Weight[n].clone().zero_()) / (p_change ** 2 + epsilon)\n",
    "        # OMEGA_add = Weight[n] / (p_change ** 2 + epsilon)\n",
    "        # OMEGA_current[n] = OMEGA_pre[n] + OMEGA_add\n",
    "        OMEGA_current[n] = OMEGA_add\n",
    "    return OMEGA_current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'argparse' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     parser \u001b[39m=\u001b[39m argparse\u001b[39m.\u001b[39mArgumentParser()\n\u001b[1;32m      3\u001b[0m     parser\u001b[39m.\u001b[39madd_argument(\u001b[39m'\u001b[39m\u001b[39m--defence\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39mstr\u001b[39m, default\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mour\u001b[39m\u001b[39m\"\u001b[39m, choices\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mour\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m      4\u001b[0m                         help\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mname of aggregation method\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m     parser\u001b[39m.\u001b[39madd_argument(\u001b[39m'\u001b[39m\u001b[39m--prate\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39mfloat\u001b[39m, nargs\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m?\u001b[39m\u001b[39m'\u001b[39m, default\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m, help\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpoison instance ratio\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'argparse' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--defence', type=str, default=\"our\", choices=[\"our\"],\n",
    "                        help=\"name of aggregation method\")\n",
    "    parser.add_argument('--prate', type=float, nargs='?', default=0.5, help=\"poison instance ratio\")\n",
    "    parser.add_argument('--Tattack', type=int, nargs='?', default=5, help=\"attack round\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    prate = args.prate\n",
    "    Ta = args.Tattack\n",
    "    frac = 1.0\n",
    "    num_clients = 100\n",
    "    batch_size = 128\n",
    "    test_BatchSize = 32\n",
    "    x_train, y_train, x_test, y_test = readdataset()\n",
    "    dataset_train = ReadData(x_train, y_train)\n",
    "    dataset_test = ReadData(x_test, y_test)\n",
    "\n",
    "    save_global_model = 'save_model.pkl'\n",
    "    # # IID Data\n",
    "    dict_clients = iid(dataset_train, num_clients, 1)\n",
    "\n",
    "    net_global = CNN_UNSW().double().to(device)\n",
    "    # net_global = MLP_UNSW().double().to(device)\n",
    "    w_glob = net_global.state_dict()\n",
    "    crit = torch.nn.CrossEntropyLoss()\n",
    "    net_global.train()\n",
    "\n",
    "    x_client = {}\n",
    "    y_client = {}\n",
    "    normal_list_client = {}\n",
    "    anomaly_list_client = {}\n",
    "    for interation in range(Ta):\n",
    "        w_locals, loss_locals = [], []\n",
    "        w_local_pre = w_glob\n",
    "        omega_locals = []\n",
    "        Y_norm = np.empty(shape=[0, 1])\n",
    "\n",
    "        num_poison_client = 0\n",
    "        for client in range(num_clients):\n",
    "            net = copy.deepcopy(net_global).to(device)\n",
    "            net_pre = copy.deepcopy(net_global.state_dict())\n",
    "            net.train()\n",
    "            mean_pre = {n: p.clone().detach() for n, p in net.named_parameters()}\n",
    "            w = {n: p.clone().detach().zero_() for n, p in net.named_parameters()}\n",
    "\n",
    "            # opt_net = torch.optim.SGD(net.parameters(), lr=0.05, momentum=0.5) #0.05\n",
    "            opt_net = torch.optim.Adam(net.parameters())\n",
    "            print('interation', interation, 'client', client)\n",
    "            idx_traindataset = DatasetSplit(dataset_train, dict_clients[client])\n",
    "            x = idx_traindataset.features.detach().cpu().numpy()\n",
    "            y = idx_traindataset.labels.detach().cpu().numpy()\n",
    "\n",
    "            anomaly_list = [i for i in range(len(y)) if y[i] != 0]\n",
    "            y[anomaly_list] = 1\n",
    "            num_attack1 = np.sum(y == 1)\n",
    "            num_poison = int(num_attack1 * 1.0)  # 0.8\n",
    "\n",
    "            if (num_poison > 0) & (num_poison_client < 40) & (interation == (Ta - 1)):  # & (interation > 0)\n",
    "                num_poison_client += 1\n",
    "                Y_norm = np.row_stack((Y_norm, [1]))  ### 异常为1\n",
    "                print('##########poison client', num_poison_client)\n",
    "                poison_client_flag = True\n",
    "                res_list = [i for i in range(len(y)) if y[i] == 1]  ###\n",
    "                res_list1 = [i for i in range(len(y)) if y[i] != 1]  ### normal\n",
    "                ###### label flipping attack\n",
    "                x1 = x[res_list1, :, :] ### clean data\n",
    "                y1 = y[res_list1] ### labels of clean data\n",
    "                y[res_list[0:num_poison]] = 0\n",
    "                x2 = x[res_list, :, :] ### poison data\n",
    "                y2 = y[res_list[0:num_poison]]\n",
    "                x = np.concatenate((x1, x2[0:int(prate * len(x2)), :, :]), axis=0)\n",
    "                y = np.concatenate((y1, y2[0:int(prate * len(x2))]), axis=0)\n",
    "                ldr_train = DataLoader(ReadData(x, y), batch_size=1024, shuffle=True)\n",
    "                epochs_per_task = 5\n",
    "                normal_list_client[client] = res_list1\n",
    "                anomaly_list_client[client] = [i for i in range(len(x1),(len(x1)+int(prate*len(x2))))]\n",
    "\n",
    "            else:\n",
    "                Y_norm = np.row_stack((Y_norm, [0]))\n",
    "                poison_client_flag = False\n",
    "                ldr_train = DataLoader(ReadData(x, y), batch_size=1024, shuffle=True)\n",
    "                epochs_per_task = 5\n",
    "                if interation == (Ta - 1):\n",
    "                    normal_list_client[client] = [i for i in range(len(y)) if y[i] == 0]\n",
    "                    anomaly_list_client[client] = [i for i in range(len(y)) if y[i] != 0]\n",
    "\n",
    "            if interation == (Ta - 1):\n",
    "                x_client[client] = x\n",
    "                y_client[client] = y\n",
    "\n",
    "            dataset_size = len(ldr_train.dataset)\n",
    "\n",
    "            for epoch in range(1, epochs_per_task + 1):\n",
    "                correct = 0\n",
    "                for batch_idx, (images, labels) in enumerate(ldr_train):\n",
    "                    old_par = {n: p.clone().detach() for n, p in net.named_parameters()}\n",
    "                    images, labels = Variable(images).to(device), Variable(labels).type(torch.LongTensor).to(device)\n",
    "                    net.zero_grad()\n",
    "                    scores = net(images)\n",
    "                    ce_loss = crit(scores, labels)\n",
    "                    loss = ce_loss\n",
    "                    grad_params = torch.autograd.grad(loss, net.parameters(), create_graph=True)\n",
    "                    pred = scores.max(1)[1]\n",
    "                    correct += pred.eq(labels.data.view_as(pred)).cpu().sum()\n",
    "                    loss.backward()\n",
    "                    opt_net.step()\n",
    "                    j = 0\n",
    "                    for n, p in net.named_parameters():\n",
    "                        # print(n,grad_params[j])\n",
    "                        w[n] -= (grad_params[j].clone().detach()) * (p.detach() - old_par[n])  ###\n",
    "                        j += 1\n",
    "                Accuracy = 100. * correct.type(torch.FloatTensor) / dataset_size\n",
    "                print('Train Epoch:{}\\tLoss:{:.4f}\\tCE_Loss:{:.4f}\\tAccuracy: {:.4f}'.format(epoch, loss.item(),\n",
    "                                                                                             ce_loss.item(), Accuracy))\n",
    "            # print(classification_report(labels.cpu().data.view_as(pred.cpu()), pred.cpu()))\n",
    "            omega = consolidate(Model=net, Weight=w, MEAN_pre=mean_pre, epsilon=0.0001)\n",
    "            omega_index = {}\n",
    "            for k in omega.keys():\n",
    "                if len(omega[k].view(-1)) > 1000:\n",
    "                    Topk = 100\n",
    "                else:\n",
    "                    Topk = int(0.1 * len(omega[k].view(-1)))\n",
    "                Topk_value_index = torch.topk(omega[k].view(-1), Topk)\n",
    "                omega_index[k] = Topk_value_index[1].tolist()\n",
    "            omega_locals.append(omega_index)\n",
    "\n",
    "            w_locals.append(copy.deepcopy(net.state_dict()))\n",
    "\n",
    "        if interation == (Ta - 1):\n",
    "            w_glob, pre_out_label = defence_our(omega_locals, w_locals, w_local_pre)\n",
    "            test_acc, test_loss = test_w(w_glob, dataset_test)\n",
    "            print('OUR Test set: Average loss: {:.4f} \\tAccuracy: {:.2f}'.format(test_loss, test_acc))\n",
    "            print('###########Filter##################')\n",
    "            normal_client_indexs = []\n",
    "            poison_client_indexs = []\n",
    "            for i in range(len(pre_out_label)):\n",
    "                if pre_out_label[i] == 1:\n",
    "                    poison_client_indexs.append(np.int(i))\n",
    "                else:\n",
    "                    normal_client_indexs.append(np.int(i))\n",
    "            model = CNN_UNSW().to(device)  #\n",
    "            model.load_state_dict(w_glob)\n",
    "            model = model.eval()\n",
    "            profiler = TorchProfiler(model)\n",
    "            layerdict = profiler.create_layers(0)  #### all layers\n",
    "            print(layerdict)\n",
    "            tp = profiler.create_profile(torch.rand(1, 1, 42), layerdict, threshold=0.5, show_progress=False,\n",
    "                                         parallel=False)\n",
    "            class_profiles = dict()\n",
    "            selected_index = dict()\n",
    "            class_profiles_mal = dict()\n",
    "            iou_normal = dict()\n",
    "            iou_threshold = dict()\n",
    "            neuron_count = {1: 12, 2: 28, 3: 3, 4: 1, 5: 1} ### topK,k={12,28,3,1,1}\n",
    "            # neuron_count = {1: 12, 2: 28, 3: 16, 4: 8, 5: 8}\n",
    "            #####predefine class_profiles,class_profiles_mal,selected_index(class path),iou_normal(threshold)\n",
    "            for cls in range(2):\n",
    "                class_profiles[cls] = dict()\n",
    "                selected_index[cls] = dict()\n",
    "                class_profiles_mal[cls] = dict()\n",
    "                iou_normal[cls] = list()\n",
    "                for layer in tp.neuron_counts:\n",
    "                    class_profiles[cls][layer] = list()\n",
    "                    selected_index[cls][layer] = Counter({})\n",
    "                    class_profiles_mal[cls][layer] = list()\n",
    "                    # print(tp.neuron_counts[layer])\n",
    "                    # print('Layer ', layer, 'the number of important neurons:', len(list(chain(*tp.neuron_counts[layer][0]))))\n",
    "            ### obtain the class paths of clean data at the clean client sides\n",
    "            for index in normal_client_indexs:\n",
    "                images = x_client[index]\n",
    "                labels = y_client[index]\n",
    "\n",
    "                normal_client_sampling_indexs = [i for i in range(len(labels))]\n",
    "                normal_client_sampling_index = np.random.choice(normal_client_sampling_indexs,\n",
    "                                                                int(len(labels) * 0.03),\n",
    "                                                                replace=False)\n",
    "\n",
    "                for i in normal_client_sampling_index:\n",
    "                    tprofiles = profiler.create_profile(torch.Tensor(images[i]).resize_(1, 1, 42),\n",
    "                                                        layerdict,\n",
    "                                                        threshold=0.5,\n",
    "                                                        show_progress=False,\n",
    "                                                        parallel=False)\n",
    "                    for layer in tprofiles.neuron_counts:\n",
    "                        if layer == 0:\n",
    "                            ###### aggregate all samples' critical neuron\n",
    "                            # class_profiles[labels[i]][layer].append(tprofiles.neuron_counts[layer])\n",
    "                            ###### aggregate the correctly-predicted samples' critical neuron\n",
    "                            if (tprofiles.neuron_counts[0])[0] == labels[i]:\n",
    "                                class_profiles[labels[i]][layer].append(tprofiles.neuron_counts[layer])\n",
    "                        else:\n",
    "                            ###### aggregate all samples' critical neuron\n",
    "                            # class_profiles[labels[i]][layer].append(list(chain(*tprofiles.neuron_counts[layer][0])))\n",
    "                            ###### aggregate the correctly-predicted samples' critical neuron\n",
    "                            if (tprofiles.neuron_counts[0])[0] == labels[i]:\n",
    "                                class_profiles[labels[i]][layer].append(\n",
    "                                    list(chain(*tprofiles.neuron_counts[layer][0])))\n",
    "                        selected_index[labels[i]][layer] += Counter(list(chain(*class_profiles[labels[i]][layer])))\n",
    "            for cls in range(2):\n",
    "                for i in range(len(class_profiles[cls][1])):\n",
    "                    ious = list()\n",
    "                    for layer in range(1, 6):\n",
    "                        ious.append(jaccard_simple(set(class_profiles[cls][layer][i]),\n",
    "                                                   set([val[0] for j, val in enumerate(\n",
    "                                                       selected_index[cls][layer].most_common(\n",
    "                                                           neuron_count[layer]))])))\n",
    "                    avg_ious = np.mean(ious)\n",
    "                    iou_normal[cls].append(avg_ious)\n",
    "            for cls in range(2):\n",
    "                # iou_threshold[cls] = np.median(np.array(iou_normal[cls]))\n",
    "                iou_threshold[cls] = np.percentile(np.array(iou_normal[cls]), 5)\n",
    "            print('iou_threshold', iou_threshold)\n",
    "            print('###############normal client done')\n",
    "            ##### detect the poisoned data at the poisoned client\n",
    "            for client in poison_client_indexs:\n",
    "\n",
    "                images = x_client[client]\n",
    "                labels = y_client[client]\n",
    "                anomaly_list = anomaly_list_client[client]\n",
    "                normal_list = normal_list_client[client]\n",
    "\n",
    "                normal_list_predicted = []\n",
    "                anomaly_list_predicted = []\n",
    "                for i in range(len(labels)):\n",
    "                    tprofiles_mal = profiler.create_profile(torch.Tensor(images[i]).resize_(1, 1, 42),\n",
    "                                                            layerdict,\n",
    "                                                            threshold=0.5,\n",
    "                                                            show_progress=False,\n",
    "                                                            parallel=False)\n",
    "                    ious = list()\n",
    "                    for layer in range(1, 6):\n",
    "                        ious.append(jaccard_simple(set(list(chain(*tprofiles_mal.neuron_counts[layer][0]))),\n",
    "                                                   set([val[0] for j, val in enumerate(\n",
    "                                                       selected_index[labels[i]][layer].most_common(\n",
    "                                                           neuron_count[layer]))])))\n",
    "                    avg_ious = np.mean(ious)\n",
    "                    if avg_ious < iou_threshold[labels[i]]:\n",
    "                        anomaly_list_predicted.append(i)\n",
    "\n",
    "                # recall_normal = set(normal_list_predicted).intersection(set(normal_list))\n",
    "                recall_anomaly = set(anomaly_list_predicted).intersection(set(anomaly_list))\n",
    "                # print('normal 0', 'recall of predicted: ', len(recall_normal) / len(normal_list_predicted),\n",
    "                #       'recall of all: ', len(recall_normal) / len(normal_list))\n",
    "                print('anomaly 1', 'recall of predicted: ', len(recall_anomaly) / len(anomaly_list_predicted),\n",
    "                      'recall of all: ', len(recall_anomaly) / len(anomaly_list), 'clean removed: ',\n",
    "                      (len(anomaly_list_predicted) - len(recall_anomaly)) / len(normal_list), 'recall_anomaly: ',\n",
    "                      len(recall_anomaly), len(anomaly_list), len(anomaly_list_predicted), len(normal_list))\n",
    "        else:\n",
    "            w_glob = FedAvg(w_locals)\n",
    "            # pass\n",
    "\n",
    "        # copy weight to net_glob\n",
    "        net_global.load_state_dict(w_glob)\n",
    "        net_global.eval()\n",
    "        acc_test, loss_test = test_img(net_global, dataset_test)\n",
    "        print(\"Testing accuracy: {:.2f}\".format(acc_test))\n",
    "\n",
    "    model_dict = net_global.state_dict()\n",
    "    test_dict = {k: w_glob[k] for k in w_glob.keys() if k in model_dict}\n",
    "    model_dict.update(test_dict)\n",
    "    net_global.load_state_dict(model_dict)\n",
    "\n",
    "    net_global.eval()\n",
    "    acc_test, loss_test = test_img(net_global, dataset_test)\n",
    "    print(\"Testing accuracy: {:.2f}\".format(acc_test))\n",
    "    #### save the model trained with the norm dataset\n",
    "    # torch.save(net_global,save_global_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
